
        <h1>Capabilities</h1>

        

<div class="heading"><h3>HPC</h3></div>
<img src="http://placehold.it/200x200&text=img" class="img-responsive pull-right thumbnail" />
<p>Axiom operates two High Performance Compute (HPC) clusters located in a data centers in Portland, Oregon and in Providence, Rhode Island. As of this writing, Axiomâ€™s HPC resources are composed of approximately 1,800 processing cores staged in a series of interconnected blade arrays as well as 2.1 PBs of storage distributed across 16 clustered storage nodes. Compute nodes and storage nodes are connected over a low latency, converging network fabric (40 Gb/Sec Infiniband). GlusterFS is employed as a storage software abstraction layer that enables clients and storage servers to exploit data transfer over Remote Direct Memory Access (RDMA) protocols. This configuration enables data throughput from the storage cluster to the compute cluster to reach speeds greater than 160 Gb/Sec in high-concurrency situations. Axiom also has a dedicated multi-braided 1 Gb/Sec high speed internet connection for large file transfers between external data centers and for high-bandwidth demands of centralized web based applications.</p>


<div class="heading"><h3>Interoperability</h3></div>

<div class="heading"><h3>Data Integration</h3></div>

<div class="heading"><h3>Visualization & UI</h3></div>

<div class="heading"><h3>Scientific Data Management & Collaboration</h3></div>












